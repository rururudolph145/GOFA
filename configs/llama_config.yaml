root_path: ""
#data_root_path: "./TAGDataset"
data_root_path: "/storage1/yinjie.tang/Active/feng.jiarui/TAGDataset"
llama_pretrain_checkpoint: "../cache_data/cache_data/model/llama-2-7b-chat-finetuned-icae_zeroweight_llama2.pt"
mistral_pretrain_checkpoint: "/project/tantra/liuhao/OFAv2/cache_data/model/mistral_7b_ft_icae.safetensors"
num_bases: 4
emb_dim: 1024
num_layers: 6
mlp_type: "gp"
dropout: 0.0
JK: "last"
lr: 0.0001
l2: 0.1
grad_clip: 0.5
num_epochs: 0
last_epochs: 0
batch_size: 1
eval_batch_size: 1
num_workers: 4
seed: 1
data_path:
offline_log: False
test_rep: 1
log_project: "ggama"
exp_name: "perp_3c"
train_sample_size: -1
eval_sample_size: -1
rwpe:
task_names:
  - arxiv
llm_name: "icae"
base_llm: "mistral7blora"
llm_max_length: 2048
llm_b_size: 1
val_interval: 1
load_texts: True
mode: "nograph"
temp: 0.1
compressed_layer: 32
max_nodes_per_hop: 5
grad_acc_step: 4
save_model:
  save: False
  steps:
  monitor: False
  time: 5
  epochs:
  top_k: -1
  last: True
load_dir: "./best_ckpt.pth"
load_model: False
last_save: True
training_precision: "bf16-mixed"
ckpt_path:
run_mode: "pretrain"
dec_lora: False
node_text: False

train_data_multiples: 1.0
selections: True

train_task_names:
  - arxiv
sample_size_per_task:
  - -1
hops:
  - 3

train_max_nodes_per_hops:
  - 10
ways: 5
save_suffix: "llm_instruct_ft"
instructs: False


eval_task_names:
  - cora_node
  - pubmed_node
  - wikics
  - products
  - expla_graph
  - fb15k237
  - scene_graph
inf_sample_size_per_task: 1.0
inf_hops:
  - 3
  - 3
  - 3
  - 3
  - -1
  - 3
  - -1

inf_max_nodes_per_hops:
  - 10
  - 10
  - 10
  - 10
  - -1
  - 10
  - -1

inf_ways:
  - 7
  - 3
  - 10
  - 10
  - 2
  - 10
  - -1

inf_instructs:
  - False
  - False
  - False
  - False
  - False
  - False
  - False

inf_selections:
  - False
  - False
  - False
  - False
  - False
  - False
  - False

