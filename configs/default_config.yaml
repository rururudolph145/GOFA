root_path: "./"  # saved pth ckpt and wandb logging
data_root_path: "./TAGDataset"  # data directory
ckpt_save_path:   # path to save deepspeed ckpt
num_layers: 5    # number of gnn layers
gnn_type: "index"
fuse_type: interleave    # determines how GNN is fused into the LLM choose from {interleave, parallel} parallel is under development.
lr: 0.0003
l2: 0.1
grad_clip: 0.5
num_epochs: 1
last_epochs: 0
batch_size: 1
eval_batch_size: 1
num_workers: 4
seed: 1
offline_log: False
log_project: "GOFA"
exp_name: "gofa"
train_sample_size: -1
eval_sample_size: -1
task_names:              # Name of the dataset trained
  - arxiv
base_llm: "mistral7b"
llm_max_length: 128
val_interval:      # validate per {val_interval} training steps.
mode: "decode"
max_nodes_per_hop: 5
grad_acc_step: 8
save_model:
  save: False
  steps:
  monitor: False
  time: 5
  epochs:
  top_k: -1
  last: True
load_dir: "./best_ckpt.pth"
load_model: False
last_save: False
training_precision: "bf16-mixed"
ckpt_path:
run_mode: "pretrain"
dec_lora: False   # Set to True to tune decoder by LoRA.